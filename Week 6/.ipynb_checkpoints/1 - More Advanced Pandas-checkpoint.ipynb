{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af439bd5",
   "metadata": {},
   "source": [
    "# More Advanced Pandas\n",
    "\n",
    "In this notebook we look at further aspects of working with Pandas DataFrames and Series, including normalising data, aggregating data, and addressing the problem of missing values in a DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c365f5-a554-422e-97de-e8b4c86559d5",
   "metadata": {},
   "source": [
    "Firstly, we will load a dataset of country-level statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd53c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset and set the index column\n",
    "df = pd.read_csv(\"world_data.csv\", index_col=\"Country\")\n",
    "# look at the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f99e9f9",
   "metadata": {},
   "source": [
    "## Frequency Tables\n",
    "\n",
    "When working with a Series with categorical values, frequency tables in Pandas provide a way of counting the frequency of different values in the Series. The function *value_counts()* returns a new Series containing counts of unique values. By default, these values are sorted.\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\n",
    "\n",
    "We could apply this to any of the categorical columns in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Region\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Landlocked\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1a051",
   "metadata": {},
   "source": [
    "We can also normalise the values, to give the relative frequencies of the unique values (i.e. the fraction of entries in the Series which have a given value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Language\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ea085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Region\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Landlocked\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f83a7",
   "metadata": {},
   "source": [
    "## Aggregating Data\n",
    "\n",
    "We can use the *groupby()* function to group data based on the values in a categorical column:\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d67b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the countries by their region value\n",
    "groups1 = df.groupby(\"Region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f37137",
   "metadata": {},
   "source": [
    "We can now apply a range of statistical operations on the groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3fd279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean of the numeric columns, per group\n",
    "groups1.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total of the numeric columns, per group\n",
    "groups1.sum(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34424daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use an alternative categorical variable to aggregate the data\n",
    "groups2 = df.groupby(\"Language\")\n",
    "groups2.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a69d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use an alternative categorical variable to aggregate the data\n",
    "groups2 = df.groupby(\"Landlocked\")\n",
    "groups2.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8654a8",
   "metadata": {},
   "source": [
    "## Cross Tabulation\n",
    "\n",
    "*Cross tabulation* allows us to quantitatively analyse the relationship between multiple variables. In Pandas, this involves counting the frequency with which values from different columns in a DataFrame co-occur.\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f25d87",
   "metadata": {},
   "source": [
    "In the simplest case, we can compare one column relative to another (our new index). For example, compare the Region and Landlocked columns, where Region will be the row index in the new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare a pair of categorical variables\n",
    "pd.crosstab(df[\"Region\"], df[\"Landlocked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare a different pair of categorical variables\n",
    "pd.crosstab(df[\"Language\"], df[\"Landlocked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare a different pair of categorical variables\n",
    "pd.crosstab(df[\"Region\"], df[\"Language\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a74cb",
   "metadata": {},
   "source": [
    "## Data Normalisation\n",
    "\n",
    "Data normalisation is a preprocessing step which is often applied to numeric columns to transform their scale or range.\n",
    "\n",
    "For instance, for country population data, we could normalise the values in this column in different ways.\n",
    "\n",
    "We could divided by the maximum value in the series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pop Norm\"] = df[\"Population\"] / df[\"Population\"].max()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd0ca2",
   "metadata": {},
   "source": [
    "Alternatively, we could subtract the mean value from each value in the column. Note that this can give negative values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6eb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pop Norm\"] = df[\"Population\"] - df[\"Population\"].mean()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d3d9a",
   "metadata": {},
   "source": [
    "A particularly common form of normalisation is to compute a *Z-score*, which involves subtracting the mean value of a variable for each value and then dividing by its standard deviation:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Standard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c135aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pop Norm\"] = (df[\"Population\"] - df[\"Population\"].mean())/df[\"Population\"].std()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553ae4f",
   "metadata": {},
   "source": [
    "Another common normalisation method is *min-max normalisation*, which rescales the range of a feature's values to [0,1], based on its minimum and maximum values. We could apply this to the life expectancy values in our dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "life_min = df[\"Life Exp\"].min()\n",
    "life_max = df[\"Life Exp\"].max()\n",
    "df[\"Life Exp Norm\"] = (df[\"Life Exp\"]-life_min)/(life_max-life_min)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a267f",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "Many real datasets have missing values, either because they exist and were not collected or because the values never existed. \n",
    "\n",
    "In the example here, we consider a different dataset representing the passenger list from the Titanic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ebf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and use the passenger Id as the row index for the DataFrame\n",
    "dft = pd.read_csv(\"titanic.csv\", index_col=\"PassengerId\")\n",
    "dft.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc35298",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945d351",
   "metadata": {},
   "source": [
    "When we load the dataset *titanic.csv* dataset, we see that some columns have many missing values - i.e. they contain the null/empty value *NaN*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many missing values per column?\n",
    "dft.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738af01",
   "metadata": {},
   "source": [
    "One option is to simply drop a feature with many missing values. So we could drop the \"Age\" column using the drop() function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a08b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.drop([\"Age\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afe544",
   "metadata": {},
   "source": [
    "However, if we expect age to play an important role, then we want to keep the column and estimate the missing values in some way. A simple approach is to fill in missing values using the mean value. We can do this using the *fillna()* function.\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_age = dft[\"Age\"].mean()\n",
    "mean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b521cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all NaN values in the Age column with the mean value\n",
    "dft[\"Age\"] = dft[\"Age\"].fillna(mean_age)\n",
    "dft.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9aacd",
   "metadata": {},
   "source": [
    "Confirm that the \"Age\" column no longer has any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06949b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
