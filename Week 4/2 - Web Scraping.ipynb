{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and Parsing HTML\n",
    "\n",
    "In cases where a web API is not available, we may have to manually retrieve HTML pages and extract the relevant data. This process is called **web scraping**.\n",
    "\n",
    "**Important considerations:**\n",
    "- Always check the website's `robots.txt` file (e.g., `website.com/robots.txt`) to see if scraping is allowed\n",
    "- Respect rate limits - don't make requests too frequently (add delays between requests)\n",
    "- Consider the website's terms of service\n",
    "- Be respectful of server resources\n",
    "- For production applications, consider using the `requests` library instead of `urllib` for better rate limiting and session management\n",
    "\n",
    "For this example, we will use the *Beautiful Soup* package version 4, which is designed to make it easier to extract information from HTML pages using Python.\n",
    "\n",
    "https://beautiful-soup-4.readthedocs.io/\n",
    "\n",
    "If the package is not already installed on your machine, you will need to install it via *pip* on the terminal:\n",
    "\n",
    "```bash\n",
    "pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the package is installed, we can import it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will download the HTML source code for our target web page at the link below. This web page contains personal details for a number of customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://mlg.ucd.ie/modules/python/customers.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "try:\n",
    "    # attempt to open the specified URL\n",
    "    response = urllib.request.urlopen(url)\n",
    "    # read the response data (bytes) and decode it into a string\n",
    "    html = response.read().decode(\"utf-8\")\n",
    "    # display the HTML string\n",
    "    print(html)\n",
    "except urllib.error.HTTPError as e:\n",
    "    print(f\"HTTP Error {e.code}: {e.reason}\")\n",
    "    html = None\n",
    "except urllib.error.URLError as e:\n",
    "    print(f\"Network Error: {e.reason}\")\n",
    "    html = None\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    html = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will try to identify all of the \\<div\\> tags with the attribute \"class\" equals to \"person\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BeautifulSoup object which contains the structure of the HTML page\n",
    "soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "# now find all the <div> tags we want\n",
    "person_matches = soup.find_all(\"div\", {\"class\": \"person\"})\n",
    "# were there any matches?    \n",
    "if person_matches:\n",
    "    print(f\"Found {len(person_matches)} person records:\")\n",
    "    for match in person_matches:\n",
    "        print(match)\n",
    "else:\n",
    "    print(\"No person records found with class 'person'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have found all of the relevant \\<div\\> tags above, we want to extract more information from each match. \n",
    "\n",
    "We can do this by finding the \\<span\\> tags inside the matched \\<div\\> tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BeautifulSoup object which contains the structure of the HTML page\n",
    "soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# list to store our results\n",
    "customers = []\n",
    "\n",
    "# find all the div tags we want\n",
    "person_matches = soup.find_all(\"div\", {\"class\": \"person\"})\n",
    "\n",
    "# any results?\n",
    "if not person_matches:\n",
    "    print(\"No person records found\")\n",
    "else:\n",
    "    print(f\"Processing {len(person_matches)} person records...\")\n",
    "    for i, match in enumerate(person_matches, 1):\n",
    "        # within each person match, find span tags and extract the text from each one\n",
    "        firstname_elem = match.find(\"span\", {\"class\": \"firstname\"})\n",
    "        lastname_elem = match.find(\"span\", {\"class\": \"lastname\"})\n",
    "        dob_elem = match.find(\"span\", {\"class\": \"dob\"})\n",
    "        \n",
    "        # check if all required elements were found\n",
    "        if not all([firstname_elem, lastname_elem, dob_elem]):\n",
    "            print(f\"Warning: Person record {i} is missing required fields\")\n",
    "            continue\n",
    "            \n",
    "        # extract text from elements\n",
    "        firstname = firstname_elem.text.strip()\n",
    "        lastname = lastname_elem.text.strip()\n",
    "        dob = dob_elem.text.strip()\n",
    "        \n",
    "        # clean the date of birth string\n",
    "        dob = dob.replace(\"DOB: \", \"\")\n",
    "        \n",
    "        # validate that we have non-empty data\n",
    "        if not all([firstname, lastname, dob]):\n",
    "            print(f\"Warning: Person record {i} contains empty fields\")\n",
    "            continue\n",
    "        \n",
    "        # add all of the information to a dictionary\n",
    "        customer = {\"firstname\": firstname, \"lastname\": lastname, \"dob\": dob}\n",
    "        customers.append(customer)\n",
    "            \n",
    "    print(f\"Successfully extracted {len(customers)} complete customer records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the data that we extracted from the HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, customer in enumerate(customers, 1):\n",
    "    print(f\"{i}. {customer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use f-string formatting to display this data more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print table header\n",
    "print(f\"{'No.':<4} {'First Name':<12} {'Last Name':<12} {'Date of Birth'}\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "# print each customer's details in a formatted row\n",
    "for i, customer in enumerate(customers, 1):\n",
    "    firstname = customer[\"firstname\"]\n",
    "    lastname = customer[\"lastname\"]\n",
    "    dob = customer[\"dob\"]\n",
    "    print(f\"{i:<4} {firstname:<12} {lastname:<12} {dob}\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
